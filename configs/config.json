{
    "project_name": "distil-logits",
    "dataset": {
        "name": "/ai-workspace/Nouran/model_distillation/DistillKit/qwen_sentiment_data.json",
        "split": "train",
        "seed": 42,
        "num_samples": 64000
    },
    "models": {
        "teacher": "Qwen/Qwen2.5-7B",
        "student": "Qwen/Qwen2.5-1.5B"
    },
    "tokenizer": {
        "max_length": 512,
        "chat_template": "{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system/nYou are a helpful assistant.<|im_end|>/n' }}{% endif %}{{'<|im_start|>' + message['role'] + '/n' + message['content'] + '<|im_end|>' + '/n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant/n' }}{% endif %}"
    },
    "training": {
        "output_dir": "./results",
        "num_train_epochs": 5,
        "per_device_train_batch_size": 1,
        "gradient_accumulation_steps": 8,
        "save_steps": 10000,
        "logging_steps": 1,
        "learning_rate": 2e-5,
        "weight_decay": 0.05,
        "warmup_ratio": 0.1,
        "lr_scheduler_type": "cosine",
        "resume_from_checkpoint": false,  
        "fp16": false,
        "bf16": true
    },
    "distillation": {
        "temperature": 2.0,
        "alpha": 0.5
    },
    "model_config": {
        "use_flash_attention": true,
        "load_in_8bit":true
    }

}
